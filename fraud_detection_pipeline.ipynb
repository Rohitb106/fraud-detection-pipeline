{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4856f1b7",
   "metadata": {},
   "source": [
    "# ðŸš€ Credit Card Fraud Detection Pipeline\n",
    "\n",
    "This notebook demonstrates **EDA, Anomaly Detection, Machine Learning Models, and Real-time Simulation** for fraud detection using the credit card dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c93ad5c",
   "metadata": {},
   "source": [
    "## ðŸ“¥ 1. Load Data Efficiently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517f9fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv(\"creditcard.csv\")\n",
    "\n",
    "print(\"Shape of dataset:\", data.shape)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9306d5f0",
   "metadata": {},
   "source": [
    "## ðŸ“Š 2. Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68678e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nClass distribution:\")\n",
    "print(data[\"Class\"].value_counts())\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(x='Class', data=data)\n",
    "plt.title(\"Fraud (1) vs Normal (0)\")\n",
    "plt.show()\n",
    "\n",
    "# Correlation Heatmap\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.heatmap(data.corr(), cmap=\"coolwarm\", cbar=False)\n",
    "plt.title(\"Feature Correlation\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa79b1",
   "metadata": {},
   "source": [
    "## ðŸ” 3. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea17a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "iso = IsolationForest(contamination=0.001, random_state=42)\n",
    "data['anomaly'] = iso.fit_predict(data.drop(['Class'], axis=1))\n",
    "\n",
    "print(\"\\nAnomaly Detection Results:\")\n",
    "print(data['anomaly'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469c9559",
   "metadata": {},
   "source": [
    "## âœ‚ï¸ 4. Train-Test Split + Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d622d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data.drop([\"Class\", \"anomaly\"], axis=1)\n",
    "y = data[\"Class\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf6d3e1",
   "metadata": {},
   "source": [
    "## âš–ï¸ 5. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8150f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f4aa7ca",
   "metadata": {},
   "source": [
    "## ðŸ¤– 6. ML Models\n",
    "We train **Logistic Regression, Decision Tree, and Neural Network**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f8a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, class_weight='balanced'),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(class_weight='balanced'),\n",
    "    \"Neural Net\": MLPClassifier(hidden_layer_sizes=(64,32), max_iter=50)\n",
    "}\n",
    "\n",
    "# Store performance\n",
    "performance = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Collect metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_test, y_pred, average='binary')\n",
    "    performance.append([name, precision, recall, f1])\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f\"Confusion Matrix - {name}\")\n",
    "    plt.show()\n",
    "\n",
    "# Create summary table\n",
    "perf_df = pd.DataFrame(performance, columns=[\"Model\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
    "print(\"\\nðŸ“Š Model Performance Summary:\")\n",
    "print(perf_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfa38ac",
   "metadata": {},
   "source": [
    "## âš¡ 7. Real-time Monitoring Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb96937",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nSimulating Real-Time Fraud Detection:\")\n",
    "for i in range(5):\n",
    "    sample = X_test[i].reshape(1, -1)\n",
    "    pred = models[\"Logistic Regression\"].predict(sample)\n",
    "    print(f\"Transaction {i+1}: Fraud\" if pred==1 else f\"Transaction {i+1}: Normal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0df17c5",
   "metadata": {},
   "source": [
    "## ðŸ“ˆ 8. Scalability Tip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2bd217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In production -> use streaming frameworks (Kafka, Spark) & incremental learning"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
